{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec # categorical feature to vectors\n",
    "from random import shuffle\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = pd.read_csv('recipe.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = recipe.dropna(subset=['CKG_NM','CKG_MTRL_CN','CKG_INBUN_NM'])\n",
    "recipe = recipe.drop_duplicates(subset=['CKG_NM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CKG_MTH_ACTO_NM 요리방법\tCKG_MTRL_ACTO_NM 요리재료 CKG_KND_ACTO_NM 요리종류 CKG_MTRL_CN 요리재료 내용\n",
    "recipe = recipe[['CKG_NM','CKG_STA_ACTO_NM', 'CKG_MTH_ACTO_NM',\t'CKG_MTRL_ACTO_NM', 'CKG_KND_ACTO_NM' ,'CKG_MTRL_CN']]\n",
    "recipe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quantity_and_unit(recipe_ingredients):\n",
    "    recipe_ingredients = recipe_ingredients.replace(\"[재료]\", \"\")\n",
    "    recipe_ingredients = recipe_ingredients.replace(\"[양념]\", \"|\")\n",
    "    ingredients_list = recipe_ingredients.split(\"|\")\n",
    "\n",
    "    cleaned_ingredients = []\n",
    "    for ingredient in ingredients_list:\n",
    "        parts = ingredient.strip().split(\" \")\n",
    "        ingredient_name = \" \".join(parts[:-1])\n",
    "        cleaned_ingredient = re.sub(r\"\\[.*?\\]\", \"\", ingredient_name).strip()\n",
    "        if cleaned_ingredient:\n",
    "            cleaned_ingredients.append(cleaned_ingredient)\n",
    "\n",
    "    return cleaned_ingredients\n",
    "\n",
    "\n",
    "def remove_empty_lists(df):\n",
    "    df = df[df[\"CKG_MTRL_CN\"].astype(bool)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df = pd.merge(df, recipe, left_on='food', right_on='CKG_NM', how='left')\n",
    "    df.drop('CKG_NM', axis=1, inplace=True)\n",
    "    df.drop('food',axis=1, inplace=True)\n",
    "    df['bmi'] = df['weight'] / (df['height']/100)**2\n",
    "    df['CKG_MTRL_CN']=df['CKG_MTRL_CN'].apply(remove_quantity_and_unit)\n",
    "    df = remove_empty_lists(df)\n",
    "    df['CKG_MTRL_CN']=df['CKG_MTRL_CN'].apply(lambda x: \" \".join(x))\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=preprocessing(train)\n",
    "test=preprocessing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daset = pd.concat([train,test],axis=0)\n",
    "cat_cols = train.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_w2v(sentences, model, num_features):\n",
    "    def _average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        n_words = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                n_words = n_words + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "        if n_words:\n",
    "            feature_vector = np.divide(feature_vector, n_words)\n",
    "        return feature_vector\n",
    "    \n",
    "    vocab = set(model.wv.index_to_key)\n",
    "    feats = [_average_word_vectors(s, model, vocab, num_features) for s in sentences]\n",
    "    return np.array(feats)\n",
    "def gen_cat2vec_sentences(data):\n",
    "    X_w2v = copy.deepcopy(data)\n",
    "    names = list(X_w2v.columns.values)\n",
    "    for c in names:\n",
    "        X_w2v[c] = X_w2v[c].fillna('unknow').astype('category')\n",
    "        X_w2v[c] = X_w2v[c].cat.rename_categories([\"%s %s\" % (c, g) for g in X_w2v[c].cat.categories])\n",
    "    X_w2v = X_w2v.values.tolist()\n",
    "    return X_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat2vec_feature  = len(cat_cols) \n",
    "n_cat2vec_window   = len(cat_cols) * 2\n",
    "\n",
    "def fit_cat2vec_model():\n",
    "    X_w2v = gen_cat2vec_sentences(daset.loc[:,cat_cols].sample(frac=0.7))\n",
    "    for i in X_w2v:\n",
    "        shuffle(i)\n",
    "    model = Word2Vec(X_w2v, vector_size=n_cat2vec_feature, window=n_cat2vec_window, seed=1)\n",
    "    return model\n",
    "\n",
    "c2v_model = fit_cat2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_c2v_matrix = apply_w2v(gen_cat2vec_sentences(daset.iloc[:len(train)][cat_cols]), c2v_model, n_cat2vec_feature)\n",
    "te_c2v_matrix = apply_w2v(gen_cat2vec_sentences(daset.iloc[len(train):][cat_cols]), c2v_model, n_cat2vec_feature)\n",
    "tr_c2v_matrix = pd.DataFrame(tr_c2v_matrix)\n",
    "te_c2v_matrix = pd.DataFrame(te_c2v_matrix)\n",
    "new_columns = [f\"cat2vec_{i+1}\" for i in range(len(tr_c2v_matrix.columns))]\n",
    "tr_c2v_matrix.columns = new_columns\n",
    "te_c2v_matrix.columns = new_columns\n",
    "train =train.drop(columns=cat_cols)\n",
    "test = test.drop(columns=cat_cols)\n",
    "train = pd.concat([train,tr_c2v_matrix], axis=1)\n",
    "test = pd.concat([test,te_c2v_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tf_columns = ['CKG_MTRL_CN']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=7, n_iter=7, random_state=42)\n",
    "\n",
    "for col in tf_columns:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(daset[col])\n",
    "    svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "    svd_df = pd.DataFrame(svd_matrix)\n",
    "    new_columns = [f\"svd_{col}_{i+1}\" for i in range(len(svd_df.columns))]\n",
    "    svd_df.columns = new_columns\n",
    "    train = pd.concat([train, svd_df.iloc[:len(train)]], axis=1)\n",
    "    \n",
    "    svd_df_te = svd_df.iloc[len(train):]\n",
    "    svd_df_te.index = test.index\n",
    "    test = pd.concat([test, svd_df_te], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(series: pd.Series) -> pd.Series:\n",
    "    my_dict = {}\n",
    "    series = series.astype(str)\n",
    "\n",
    "    for idx, value in enumerate(sorted(series.unique())):\n",
    "        my_dict[value] = idx\n",
    "    series = series.map(my_dict)\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['gender', 'CKG_STA_ACTO_NM', 'CKG_MTH_ACTO_NM', 'CKG_MTRL_ACTO_NM', 'CKG_KND_ACTO_NM']\n",
    "for col in label_columns:\n",
    "    daset[col] = label_encoding(daset[col])\n",
    "train[label_columns] = daset.iloc[:len(train)][label_columns]\n",
    "test[label_columns] = daset.iloc[len(train):][label_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "x_train, y_train = train.drop(\"portion\", axis=1), train[\"portion\"]\n",
    "x_test, y_test = test.drop(\"portion\", axis=1), test[\"portion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor \n",
    "\n",
    "def cv_regression(model, k_fold):\n",
    "    # K-fold 교차 검증 설정\n",
    "    k_folds = k_fold \n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    test_prediction = []\n",
    "    k_rmse_score = []\n",
    "    models = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(x_train, y_train)):\n",
    "\n",
    "        # 훈련 데이터와 검증 데이터 분할\n",
    "        x_train_fold, x_val_fold = x_train.iloc[train_idx], x_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # 모델 학습\n",
    "        model.fit(x_train_fold.fillna(0), y_train_fold)\n",
    "        models.append(model)\n",
    "\n",
    "        # 검증 데이터에 대한 예측\n",
    "        y_pred = model.predict(x_val_fold.fillna(0))\n",
    "        y_pred = np.round(y_pred,1)\n",
    "        # 모델 평가 \n",
    "        rmse = mean_squared_error(y_val_fold, y_pred)**0.5\n",
    "        print(f\"Fold {fold+1} - RMSE: {rmse}\")\n",
    "        k_rmse_score.append(rmse)\n",
    "\n",
    "        # 테스트 데이터에 대한 예측\n",
    "        test_prediction.append(model.predict(x_test.fillna(0)))\n",
    "\n",
    "    return k_rmse_score, test_prediction, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'learning_rate' : trial.suggest_float('learning_rate', .001, .1, log = True),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample' : trial.suggest_float('subsample', .5, 1),\n",
    "        'min_child_weight' : trial.suggest_float('min_child_weight', .1, 15, log = True),\n",
    "        'reg_lambda' : trial.suggest_float('reg_lambda', .1, 20, log = True),\n",
    "        'reg_alpha' : trial.suggest_float('reg_alpha', .1, 10, log = True),\n",
    "        'n_estimators' : 1000,\n",
    "        'random_state' : seed,\n",
    "    }\n",
    "    optuna_model = make_pipeline(\n",
    "            LGBMRegressor(**params, verbosity=-1)\n",
    "        )\n",
    "    optuna_score, _, __ = cv_regression(optuna_model, 5)\n",
    "    return np.mean(optuna_score)\n",
    "lgb_study = optuna.create_study(direction = 'minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study.optimize(lgb_objective, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = lgb_study.best_trial\n",
    "lgb_params = trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'eta' : trial.suggest_float('eta', .001, .1, log = True),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 2, 30),\n",
    "        'subsample' : trial.suggest_float('subsample', .5, 1),\n",
    "        'colsample_bytree' : trial.suggest_float('colsample_bytree', .1, 1),\n",
    "        'min_child_weight' : trial.suggest_float('min_child_weight', .1, 20, log = True),\n",
    "        'reg_lambda' : trial.suggest_float('reg_lambda', .01, 20, log = True),\n",
    "        'reg_alpha' : trial.suggest_float('reg_alpha', .01, 10, log = True),\n",
    "        'n_estimators' : 1000,\n",
    "        'random_state' : seed,\n",
    "        'tree_method' : 'hist',\n",
    "    }\n",
    "    \n",
    "    optuna_model = make_pipeline(\n",
    "        XGBRegressor(**params, verbosity=0)  \n",
    "    )\n",
    "    \n",
    "    optuna_score, _, _ = cv_regression(optuna_model, 5)\n",
    "    return np.mean(optuna_score)\n",
    "\n",
    "xgb_study = optuna.create_study(direction = 'minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_study.optimize(xgb_objective, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = xgb_study.best_trial\n",
    "xgb_params = trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_objective(trial):\n",
    "    params = {\n",
    "        \"iterations\": 1000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        'random_state' : seed,\n",
    "    }\n",
    "    optuna_model = make_pipeline(\n",
    "        CatBoostRegressor(**params, verbose=0) \n",
    "    )\n",
    "    \n",
    "    optuna_score, _, _ = cv_regression(optuna_model, 5)\n",
    "    return np.mean(optuna_score)\n",
    "\n",
    "cat_study = optuna.create_study(direction='minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_study.optimize(cat_objective, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = cat_study.best_trial\n",
    "cat_params = trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_model = make_pipeline(\n",
    "            LGBMRegressor(**lgb_params, n_estimators= 1000, random_state=seed, verbosity=-1)\n",
    "        )\n",
    "optuna_score, lgb_pred, lgb_models = cv_regression(optuna_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_model = make_pipeline(\n",
    "            XGBRegressor(**xgb_params, n_estimators= 1000, random_state=seed, verbosity=0)\n",
    "        )\n",
    "optuna_score, xgb_pred, xgb_models = cv_regression(optuna_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_model = make_pipeline(\n",
    "            CatBoostRegressor(**cat_params, n_estimators= 1000, random_state=seed, verbose=0)\n",
    "        )\n",
    "optuna_score, cat_pred, cat_models = cv_regression(optuna_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lgbm rmse : ', mean_squared_error(y_test,np.mean(lgb_pred,axis=0)**0.5))\n",
    "print('xgb rmse : ', mean_squared_error(y_test,np.mean(xgb_pred,axis=0)**0.5))\n",
    "print('cat rmse : ', mean_squared_error(y_test,np.mean(cat_pred,axis=0)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "def save_models(models, name, model_dir='models/'):\n",
    "    for i, model in enumerate(models):\n",
    "        filename = model_dir + f'{name}_{i}.pkl'\n",
    "        joblib.dump(model, filename)\n",
    "        print(f'Model {i} saved as {filename}')\n",
    "        \n",
    "def load_models(model_dir='models/', name='', n_models=5):\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        filename = model_dir + f'{name}_{i}.pkl'\n",
    "        model = joblib.load(filename)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(lgb_models,'lgb')\n",
    "save_models(xgb_models,'xgb')\n",
    "save_models(cat_models,'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_models = load_models(name='lgb')\n",
    "xgb_models = load_models(name='xgb')\n",
    "cat_models = load_models(name='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(models, test_data):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        prediction = model.predict(test_data)\n",
    "        predictions.append(prediction)\n",
    "    return np.mean(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_test_data(lgb_models, x_test)\n",
    "test_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecoportion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
